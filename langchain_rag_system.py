# advanced_rag_system.py
"""
é«˜çº§åˆåŒç®¡ç†ç³»ç»Ÿ - ä½¿ç”¨LangChainå®ç°
æ”¯æŒPDFè§£æã€åˆåŒæ€»ç»“ã€æ™ºèƒ½é—®ç­”ç­‰åŠŸèƒ½
"""

import os
from typing import List, Dict, Optional, Tuple
import hashlib
import pickle
from datetime import datetime
import pandas as pd
# LangChainæ ¸å¿ƒç»„ä»¶
from langchain_community.document_loaders.pdf import PyMuPDFLoader, PDFPlumberLoader
# å¦‚åªç”¨å…¶ä¸€ï¼Œä¹Ÿå¯åªç•™ä¸€ä¸ª

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import (
    RetrievalQA, 
    ConversationalRetrievalChain,
    LLMChain
)
from langchain_community.llms import OpenAI
from langchain_community.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain

from langchain.retrievers.contextual_compression import ContextualCompressionRetriever

from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_community.callbacks.manager import get_openai_callback

# å·¥å…·ç±»
import numpy as np
from pathlib import Path
import json
from dotenv import load_dotenv
load_dotenv()
class AdvancedContractRAG:
    """
    é«˜çº§åˆåŒRAGç³»ç»Ÿ
    ç‰¹æ€§ï¼š
    - å¼ºå¤§çš„PDFè§£æï¼ˆæ”¯æŒå¤æ‚æ ¼å¼ï¼‰
    - æ™ºèƒ½æ–‡æ¡£åˆ†å—
    - è¯­ä¹‰å‘é‡æœç´¢
    - åˆåŒè‡ªåŠ¨æ€»ç»“
    - å¯¹è¯å†å²è®°å¿†
    - å¤šè¯­è¨€æ”¯æŒ
    """
    
    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo", language: str = "en"):
        """
        åˆå§‹åŒ–é«˜çº§RAGç³»ç»Ÿ
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹ (gpt-3.5-turbo, gpt-4ç­‰)
            language: è¯­è¨€è®¾ç½® (en, zhç­‰)
        """
        self.api_key = api_key
        self.model = model
        self.language = language
        
        
        # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        proxies = os.getenv("HTTPS_PROXY") or os.getenv("HTTP_PROXY")
        if proxies:
            # å¯¹äºéœ€è¦ä»£ç†çš„æƒ…å†µï¼Œå¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ["OPENAI_PROXY"] = proxies

        # åˆå§‹åŒ–OpenAIç»„ä»¶ - ä½¿ç”¨å…¼å®¹çš„å‚æ•°
        self.llm = ChatOpenAI(
            temperature=0,  # é™ä½åˆ°0æé«˜é€Ÿåº¦
            model_name=model,  # ä½¿ç”¨ model_name è€Œä¸æ˜¯ model
            openai_api_key=api_key,
            max_tokens=400,  # å‡å°‘tokenæ•°é‡ä»¥åŠ å¿«å“åº”
            request_timeout=30,  # å‡å°‘è¶…æ—¶æ—¶é—´
            streaming=False  # ç¦ç”¨æµå¼ä¼ è¾“ä»¥è·å¾—å®Œæ•´å“åº”
        )
        
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=api_key
        )
        
        # æ–‡æœ¬åˆ†å‰²å™¨ - æ™ºèƒ½åˆ†å—ï¼ˆä¼˜åŒ–ï¼šå‡å°å—å¤§å°æé«˜æ£€ç´¢é€Ÿåº¦ï¼‰
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,        # å‡å°å—å¤§å°ä»¥åŠ å¿«æ£€ç´¢
            chunk_overlap=100,     # å‡å°‘é‡å ä»¥æé«˜é€Ÿåº¦
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]  # æ”¯æŒä¸­è‹±æ–‡
        )
        
        # å‘é‡å­˜å‚¨
        self.vectorstore = None
        self.retriever = None
        
        # å¯¹è¯è®°å¿†ï¼ˆä¼˜åŒ–ï¼šå‡å°‘è®°å¿†è½®æ•°ä»¥åŠ å¿«å¤„ç†ï¼‰
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            input_key="question",   # âœ… å‘Šè¯‰ memoryï¼šè¾“å…¥å­—æ®µå« question
            output_key="answer",
            return_messages=True,
            k=3  # å‡å°‘åˆ°3è½®å¯¹è¯ä»¥æé«˜é€Ÿåº¦
        )
        
        # å­˜å‚¨å·²åŠ è½½çš„æ–‡æ¡£
        self.documents = {}
        self.contract_metadata = {}
        
        # ç¼“å­˜ç›®å½•
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)

    def _normalize_text(self, text: str) -> str:
        """
        æ ‡å‡†åŒ–æ–‡æœ¬ä¸­çš„Unicodeå­—ç¬¦
        å°†æ•°å­¦æ–œä½“ç­‰ç‰¹æ®Šå­—ç¬¦è½¬æ¢ä¸ºæ™®é€šASCII
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ–‡æœ¬
        """
        import unicodedata
        
        # æ•°å­¦æ–œä½“å­—ç¬¦æ˜ å°„ (U+1D400-U+1D7FF)
        math_italic_lowercase = {
            'ğ‘': 'a', 'ğ‘': 'b', 'ğ‘': 'c', 'ğ‘‘': 'd', 'ğ‘’': 'e', 'ğ‘“': 'f',
            'ğ‘”': 'g', 'ğ˜©': 'h', 'ğ‘–': 'i', 'ğ‘—': 'j', 'ğ‘˜': 'k', 'ğ‘™': 'l',
            'ğ‘š': 'm', 'ğ‘›': 'n', 'ğ‘œ': 'o', 'ğ‘': 'p', 'ğ‘': 'q', 'ğ‘Ÿ': 'r',
            'ğ‘ ': 's', 'ğ‘¡': 't', 'ğ‘¢': 'u', 'ğ‘£': 'v', 'ğ‘¤': 'w', 'ğ‘¥': 'x',
            'ğ‘¦': 'y', 'ğ‘§': 'z',
        }
        
        math_italic_uppercase = {
            'ğ´': 'A', 'ğµ': 'B', 'ğ¶': 'C', 'ğ·': 'D', 'ğ¸': 'E', 'ğ¹': 'F',
            'ğº': 'G', 'ğ»': 'H', 'ğ¼': 'I', 'ğ½': 'J', 'ğ¾': 'K', 'ğ¿': 'L',
            'ğ‘€': 'M', 'ğ‘': 'N', 'ğ‘‚': 'O', 'ğ‘ƒ': 'P', 'ğ‘„': 'Q', 'ğ‘…': 'R',
            'ğ‘†': 'S', 'ğ‘‡': 'T', 'ğ‘ˆ': 'U', 'ğ‘‰': 'V', 'ğ‘Š': 'W', 'ğ‘‹': 'X',
            'ğ‘Œ': 'Y', 'ğ‘': 'Z',
        }
        
        # å…¶ä»–ç‰¹æ®Šå­—ç¬¦
        special_chars = {
            'â„': 'h',   # PLANCK CONSTANT
            'â„˜': 'P',   # SCRIPT CAPITAL P
            'â„“': 'l',   # SCRIPT SMALL L
            'â„¯': 'e',   # SCRIPT SMALL E
            'â„Š': 'g',   # SCRIPT SMALL G
            'â„´': 'o',   # SCRIPT SMALL O
        }
        
        # åˆå¹¶æ‰€æœ‰æ˜ å°„
        char_map = {**math_italic_lowercase, **math_italic_uppercase, **special_chars}
        
        # é€å­—ç¬¦æ›¿æ¢
        result = []
        for char in text:
            result.append(char_map.get(char, char))
        
        text = ''.join(result)

        text = text.replace('$', 'S$')  # æˆ–è€…ç›´æ¥åˆ é™¤è¿™ä¸€è¡Œ


        # Unicodeæ ‡å‡†åŒ–ï¼ˆNFKDï¼šå…¼å®¹åˆ†è§£ï¼‰
        text = unicodedata.normalize('NFKD', text)
        
        # å¯é€‰ï¼šç§»é™¤ä¸å¯è§æ§åˆ¶å­—ç¬¦
        text = ''.join(c for c in text if c.isprintable() or c.isspace())
        
        return text
    
    def _normalize_documents(self, documents):
        """
        æ ‡å‡†åŒ–æ–‡æ¡£åˆ—è¡¨ä¸­çš„æ‰€æœ‰æ–‡æœ¬
        
        Args:
            documents: LangChain Documentå¯¹è±¡åˆ—è¡¨
            
        Returns:
            æ ‡å‡†åŒ–åçš„Documentå¯¹è±¡åˆ—è¡¨
        """
        from langchain.schema import Document
        
        normalized_docs = []
        for doc in documents:
            normalized_text = self._normalize_text(doc.page_content)
            normalized_doc = Document(
                page_content=normalized_text,
                metadata=doc.metadata
            )
            normalized_docs.append(normalized_doc)
        
        return normalized_docs
       
    def load_pdf(self, pdf_path: str, use_cache: bool = True) -> Dict:
        """
        åŠ è½½å¹¶è§£æPDFæ–‡ä»¶
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            åŒ…å«è§£æç»“æœçš„å­—å…¸
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            return {"success": False, "error": f"File not found: {pdf_path}"}
        
        # ç¡®ä¿åªæœ‰ä¸€ä¸ªæ–‡æ¡£ï¼ˆæ–°å¢ï¼‰
        if self.ensure_single_document(str(pdf_path)):
            # å¦‚æœæ˜¯åŒä¸€ä¸ªæ–‡ä»¶ä¸”å·²åŠ è½½ï¼Œç›´æ¥è¿”å›
            return {
                "success": True, 
                "message": "Document already loaded",
                "stats": self.contract_metadata.get(str(pdf_path), {})
            }
    
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(pdf_path)
        cache_path = self.cache_dir / f"{cache_key}.pkl"
        
        if use_cache and cache_path.exists():
            print(f"ğŸ“‚ Loading from cache: {cache_path}")
            with open(cache_path, 'rb') as f:
                cached_data = pickle.load(f)
                self.documents[str(pdf_path)] = cached_data['documents']
                self._rebuild_vectorstore()
                return {"success": True, "message": "Loaded from cache", "stats": cached_data['stats']}
        
        print(f"ğŸ“„ Loading PDF: {pdf_path}")
        
        # å°è¯•å¤šç§PDFåŠ è½½å™¨
        documents = None
   
           
        # æ–¹æ³•1: PDFPlumber (æœ€å¥½çš„è¡¨æ ¼æ”¯æŒ)
        try:
            loader = PDFPlumberLoader(str(pdf_path))
            documents = loader.load()
            loader_used = "PDFPlumber"
            print(f"âœ… Successfully loaded with PDFPlumber")
        except Exception as e:
            print(f"âš ï¸ PDFPlumber failed: {e}")
        
        # æ–¹æ³•2: PyMuPDF (æœ€å‡†ç¡®çš„æ–‡æœ¬æå–)
        if documents is None or len(documents) == 0:
            try:
                loader = PyMuPDFLoader(str(pdf_path))
                documents = loader.load()
                loader_used = "PyMuPDF"
                print(f"âœ… Successfully loaded with PyMuPDF")
            except Exception as e:
                print(f"âš ï¸ PyMuPDF failed: {e}")
        
        
        if documents is None or len(documents) == 0:
            return {"success": False, "error": "Failed to extract text from PDF"}
        

        documents = self._normalize_documents(documents)
        
        # æå–å…ƒæ•°æ®
        total_pages = len(documents)
        total_text = " ".join([doc.page_content for doc in documents])
        total_chars = len(total_text)
        
        # æ™ºèƒ½æ–‡æ¡£åˆ†å—
        split_documents = self.text_splitter.split_documents(documents)
        
        # ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®
        for i, doc in enumerate(split_documents):
            doc.metadata.update({
                "source": str(pdf_path),
                "chunk_id": i,
                "loader": loader_used,
                "timestamp": datetime.now().isoformat()
            })
        
        # å­˜å‚¨æ–‡æ¡£
        self.documents[str(pdf_path)] = split_documents
        
        # æ›´æ–°å‘é‡å­˜å‚¨
        self._rebuild_vectorstore()
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "file": pdf_path.name,
            "pages": total_pages,
            "characters": total_chars,
            "chunks": len(split_documents),
            "loader": loader_used,
            "avg_chunk_size": total_chars // len(split_documents) if split_documents else 0
        }
        
        # ç¼“å­˜å¤„ç†ç»“æœ
        if use_cache:
            cache_data = {
                "documents": split_documents,
                "stats": stats,
                "timestamp": datetime.now().isoformat()
            }
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            print(f"ğŸ’¾ Cached to: {cache_path}")
        
        # å­˜å‚¨åˆåŒå…ƒæ•°æ®
        self.contract_metadata[str(pdf_path)] = stats
        
        return {"success": True, "message": f"Successfully loaded {pdf_path.name}", "stats": stats}
    
    def _get_cache_key(self, file_path: Path) -> str:
        """ç”Ÿæˆæ–‡ä»¶ç¼“å­˜é”®"""
        stat = file_path.stat()
        unique_str = f"{file_path}_{stat.st_size}_{stat.st_mtime}"
        return hashlib.md5(unique_str.encode()).hexdigest()
    
    def _rebuild_vectorstore(self):
        """é‡å»ºå‘é‡å­˜å‚¨"""
        all_documents = []
        for docs in self.documents.values():
            all_documents.extend(docs)
        
        if all_documents:
            print(f"ğŸ”„ Building vector store with {len(all_documents)} chunks...")
            self.vectorstore = FAISS.from_documents(
                all_documents,
                self.embeddings
            )
            
            # åˆ›å»ºå¢å¼ºæ£€ç´¢å™¨ï¼ˆä¼˜åŒ–ï¼šå‡å°‘æ£€ç´¢æ•°é‡ä»¥åŠ å¿«é€Ÿåº¦ï¼‰
            self.retriever = self.vectorstore.as_retriever(
                search_type="similarity",  # ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢ï¼Œé€Ÿåº¦æ›´å¿«
                search_kwargs={
                    "k": 8,  # è¿”å›5ä¸ªæœ€ç›¸å…³çš„å—
                    #"fetch_k": 10  # å…ˆè·å–10ä¸ªå€™é€‰
                }
            )
            print(f"âœ… Vector store ready")
    
    def summarize_contract(self, pdf_path: Optional[str] = None, 
                          summary_type: str = "comprehensive") -> str:
        """
        ç”ŸæˆåˆåŒæ‘˜è¦
        
        Args:
            pdf_path: æŒ‡å®šPDFè·¯å¾„ï¼ŒNoneåˆ™æ€»ç»“æ‰€æœ‰å·²åŠ è½½æ–‡æ¡£
            summary_type: æ‘˜è¦ç±»å‹
                - "brief": ç®€çŸ­æ‘˜è¦ï¼ˆ1-2æ®µï¼‰
                - "comprehensive": å…¨é¢æ‘˜è¦ï¼ˆåŒ…å«æ‰€æœ‰å…³é”®æ¡æ¬¾ï¼‰
                - "key points": å…³é”®ç‚¹åˆ—è¡¨
                
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        if not self.documents:
            return "No documents loaded. Please load a contract first."
        
        # è·å–è¦æ€»ç»“çš„æ–‡æ¡£
        """  if pdf_path and pdf_path in self.documents:
            docs_to_summarize = self.documents[pdf_path]
        else:
            docs_to_summarize = []
            for docs in self.documents.values():
                docs_to_summarize.extend(docs)
        """
        if pdf_path and pdf_path in self.documents:
            docs_to_summarize = self.documents[pdf_path]
        else:
        # æœ€è¿‘ä¸€ä»½
            last_key = next(reversed(self.documents.keys()))
            docs_to_summarize = self.documents[last_key]
        
        docs_to_summarize = self._normalize_documents(docs_to_summarize)
        
        # ä¼˜åŒ–ï¼šé™åˆ¶æ–‡æ¡£å—æ•°é‡ä»¥æé«˜é€Ÿåº¦ï¼ˆæ‰€æœ‰ç±»å‹ç»Ÿä¸€ï¼‰
        if len(docs_to_summarize) > 10:
            docs_to_summarize = docs_to_summarize[:10]
            print(f"ğŸ“„ Optimized: Using first 10 chunks for faster processing")
        
        # æ ¹æ®ç±»å‹é€‰æ‹©æç¤ºæ¨¡æ¿
        if summary_type == "brief":
            prompt_template = """
            Provide a brief but informative summary of this rental contract in 2-3 short paragraphs.
            
            Paragraph 1: Property details, parties involved, and lease duration
            Paragraph 2: Financial terms - rent amount, payment schedule, security deposit, and any fees
            Paragraph 3: Key responsibilities and important rules/restrictions
            
            Use specific numbers and dates from the contract.
            
            Contract content:
            {text}
            
            Brief Summary:
            """
        elif summary_type == "key points":
            prompt_template = """
            Extract key points from this rental contract in a numbered list.
            Be concise - one line per point:
            1. Rent amount and due date
            2. Lease start and end dates
            3. Security deposit amount
            4. Tenant maintenance duties
            5. Landlord maintenance duties
            6. Termination notice period
            7. Key restrictions (pets, smoking, etc.)
            
            Contract content:
            {text}
            
            Key Points:
            """
        else:  # comprehensive
            prompt_template = """
            Provide a concise comprehensive summary of this rental contract in 300 words or less.
            Use structured format with key details only.
            
            Format:
            **PARTIES & PROPERTY**
            [Names, address in 1 line]
            
            **FINANCIAL TERMS**
            â€¢ Rent: [amount/month]
            â€¢ Deposit: [amount]
            â€¢ Payment: [due date]
            â€¢ Late fee: [amount/terms if any]
            â€¢ Fees: [if any]
            
            **LEASE PERIOD**
            [Start] to [End] | Renewal: [terms]
            
            **RESPONSIBILITIES**
            Landlord: [key duties]
            Tenant: [key duties]
            
            **RULES & RESTRICTIONS**
            [Bullet list of key rules]
            â€¢ Pets: [policy]
            
            **TERMINATION**
            Notice: [period] | Conditions: [brief]
            
            **SPECIAL TERMS**
            [Any unique clauses, if none write "None"]
            
            Be brief and use exact numbers/dates from contract.
            
            Contract content:
            {text}
            
            Comprehensive Summary:
            """
        
        # åˆ›å»ºæ€»ç»“é“¾ - ç»Ÿä¸€ä½¿ç”¨stuffç­–ç•¥ï¼ˆæœ€å¿«ï¼‰
        prompt = PromptTemplate(template=prompt_template, input_variables=["text"])
        
        with get_openai_callback() as cb:
            # ç»Ÿä¸€ä½¿ç”¨stuffç­–ç•¥ï¼Œæœ€å¿«é€Ÿ
            chain = load_summarize_chain(
                self.llm,
                chain_type="stuff",
                prompt=prompt
            )
            
            summary = chain.run(docs_to_summarize)
            
            print(f"ğŸ“Š Summary generated - Tokens used: {cb.total_tokens}, Cost: ${cb.total_cost:.4f}")
        
        return summary
    
    def ask_question(self, question: str, use_compression: bool = False) -> Dict:
        """
        ä¼˜åŒ–ç‰ˆé—®ç­”ï¼šé»˜è®¤å…³é—­å‹ç¼©ä»¥æé«˜é€Ÿåº¦
        """
        if not self.vectorstore:
            return {
                "answer": "No contract loaded. Please upload a PDF contract first.",
                "sources": []
            }

        # é€‰æ‹©æ£€ç´¢å™¨ï¼ˆå‹ç¼©ä¼šæ˜¾è‘—é™ä½é€Ÿåº¦ï¼Œé»˜è®¤å…³é—­ï¼‰
        if use_compression:
            compressor = LLMChainExtractor.from_llm(self.llm)
            retriever = ContextualCompressionRetriever(
                base_compressor=compressor,
                base_retriever=self.retriever
            )
        else:
            retriever = self.retriever

        # â­ ä¸æŠŠ memory äº¤ç»™é“¾ï¼›æ”¹ä¸ºæ‰‹åŠ¨ä¼  chat_history
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            return_source_documents=True,
            verbose=False,
            output_key="answer"  # ä¸»è¾“å‡ºä¸º answer
        )

        # ä»æœ¬åœ° memory å–å†å²ï¼Œä¼ ç»™é“¾ï¼ˆlist[BaseMessage] / list[str] å‡å¯ï¼‰
        try:
            history_vars = self.memory.load_memory_variables({})
            chat_history = history_vars.get("chat_history", [])
        except Exception:
            chat_history = []

        # æ‰§è¡Œ
        with get_openai_callback() as cb:
            result = qa_chain.invoke({
                "question": question,
                "chat_history": chat_history
            })

        # æ‰‹åŠ¨æŠŠæœ¬è½®é—®ç­”å†™å› memoryï¼ˆåªå­˜ question/answerï¼‰
        try:
            self.memory.save_context({"question": question}, {"answer": result.get("answer", "")})
        except Exception:
            pass

        # â­ æ”¹è¿›çš„æ¥æºåŒ¹é…é€»è¾‘ï¼šæ ¹æ®ç­”æ¡ˆå†…å®¹ç­›é€‰æœ€ç›¸å…³çš„æ¥æº
        answer_text = result.get("answer", "")
        source_documents = result.get("source_documents", [])
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®ç­”æ¡ˆæˆ–æ¥æºï¼Œè¿”å›ç©º
        if not answer_text or not source_documents:
            return {
                "answer": answer_text,
                "sources": [],
                "tokens_used": cb.total_tokens if "cb" in locals() else 0
            }
        
        # æå–ç­”æ¡ˆä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆæ•°å­—ã€é‡‘é¢ã€æ—¥æœŸç­‰ï¼‰
        import re
        answer_keywords = set()
        
        # æå–æ•°å­—ï¼ˆåŒ…æ‹¬é‡‘é¢ï¼‰
        numbers = re.findall(r'\$?\d+[,\d]*\.?\d*', answer_text)
        answer_keywords.update(numbers)
        
        # æå–æ—¥æœŸ
        dates = re.findall(r'\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b|\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4}\b', answer_text, re.IGNORECASE)
        answer_keywords.update(dates)
        
        # æå–ç­”æ¡ˆä¸­çš„é‡è¦è¯æ±‡ï¼ˆé•¿åº¦>3çš„å•è¯ï¼Œæ’é™¤å¸¸è§è¯ï¼‰
        stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'out', 'day', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'who', 'will', 'with'}
        words = re.findall(r'\b[A-Za-z]{4,}\b', answer_text.lower())
        answer_keywords.update([w for w in words if w not in stopwords])
        
        # å¯¹æ¯ä¸ªæ¥æºæ–‡æ¡£è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        scored_sources = []
        for doc in source_documents:
            content = doc.page_content if doc.page_content else ""
            content_lower = content.lower()
            
            # è®¡ç®—åŒ¹é…åˆ†æ•°
            score = 0
            matched_keywords = []
            
            for keyword in answer_keywords:
                keyword_lower = keyword.lower()
                # ç²¾ç¡®åŒ¹é…å¾—é«˜åˆ†
                if keyword_lower in content_lower:
                    # æ•°å­—å’Œé‡‘é¢åŒ¹é…å¾—æ›´é«˜åˆ†
                    if re.match(r'\$?\d+', keyword):
                        score += 10
                    # æ—¥æœŸåŒ¹é…å¾—é«˜åˆ†
                    elif re.search(r'\d{1,2}[/-]\d{1,2}', keyword):
                        score += 8
                    else:
                        score += 2
                    matched_keywords.append(keyword)
            
            # åªä¿ç•™æœ‰åŒ¹é…çš„æ–‡æ¡£
            if score > 0:
                scored_sources.append({
                    "score": score,
                    "content": content,
                    "source": doc.metadata.get("source", "Unknown"),
                    "page": doc.metadata.get("page", "Unknown"),
                    "matched_keywords": matched_keywords
                })
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ¥æºï¼Œè¿”å›åˆ†æ•°æœ€é«˜çš„å‰3ä¸ªåŸå§‹æ¥æº
        if not scored_sources:
            sources = []
            for doc in source_documents[:3]:  # æœ€å¤š3ä¸ª
                sources.append({
                    "content": doc.page_content if doc.page_content else "",
                    "source": doc.metadata.get("source", "Unknown"),
                    "page": doc.metadata.get("page", "Unknown"),
                    "similarity_score": 0  # æ²¡æœ‰åŒ¹é…æ—¶åˆ†æ•°ä¸º0
                })
        else:
            # æŒ‰åˆ†æ•°æ’åº
            scored_sources.sort(key=lambda x: x["score"], reverse=True)
            
            # åªä¿ç•™ç›¸ä¼¼åº¦åˆ†æ•°>=20çš„æ¥æº
            filtered_sources = [src for src in scored_sources if src["score"] >= 20]
            
            sources = []
            
            # å¦‚æœè¿‡æ»¤åæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ¥æºï¼Œè¿”å›åˆ†æ•°æœ€é«˜çš„å‰3ä¸ªåŸå§‹æ¥æº
            if not filtered_sources:
                for doc in source_documents[:3]:  # æœ€å¤š3ä¸ª
                    sources.append({
                        "content": doc.page_content if doc.page_content else "",
                        "source": doc.metadata.get("source", "Unknown"),
                        "page": doc.metadata.get("page", "Unknown"),
                        "similarity_score": 0  # æ²¡æœ‰åŒ¹é…æ—¶åˆ†æ•°ä¸º0
                    })
            else:
                # å¦‚æœè¿‡æ»¤åçš„æ¥æºå°‘äº3ä¸ªï¼Œåªè¿”å›æœ€ç›¸å…³çš„ä¸€ä¸ª
                # å¦‚æœæœ‰3ä¸ªæˆ–æ›´å¤šï¼Œåˆ™è¿”å›å‰3ä¸ª
                num_sources = 1 if len(filtered_sources) < 3 else 3
                
                for src in filtered_sources[:num_sources]:
                    sources.append({
                        "content": src["content"],
                        "source": src["source"],
                        "page": src["page"],
                        "similarity_score": src["score"]
                    })
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ˜ç¡®ä¿¡æ¯ï¼Œåªè¿”å›1ä¸ªæ¥æº
        # æ£€æµ‹ç­”æ¡ˆæ˜¯å¦åªåŒ…å«å•ä¸€ä¿¡æ¯ï¼ˆä¾‹å¦‚åªæœ‰ä¸€ä¸ªæ•°å­—æˆ–æ—¥æœŸï¼‰
        if len(numbers) == 1 and len(dates) == 0 and len(sources) > 1:
            # åªä¿ç•™åˆ†æ•°æœ€é«˜çš„é‚£ä¸ª
            sources = sources[:1]
        
        return {
            "answer": answer_text,
            "sources": sources,
            "tokens_used": cb.total_tokens if "cb" in locals() else 0
        }

    
    
    def compare_contracts(self, pdf_path1: str, pdf_path2: str) -> str:
        """
        æ¯”è¾ƒä¸¤ä»½åˆåŒçš„å·®å¼‚
        
        Args:
            pdf_path1: ç¬¬ä¸€ä»½åˆåŒè·¯å¾„
            pdf_path2: ç¬¬äºŒä»½åˆåŒè·¯å¾„
            
        Returns:
            æ¯”è¾ƒç»“æœ
        """
        if pdf_path1 not in self.documents or pdf_path2 not in self.documents:
            return "Both contracts must be loaded first."
        
        prompt = PromptTemplate(
            template="""
            Compare these two rental contracts and highlight the key differences:
            
            Contract 1:
            {contract1}
            
            Contract 2:
            {contract2}
            
            Provide a detailed comparison covering:
            1. Rent amount differences
            2. Lease term differences
            3. Deposit variations
            4. Different rules or restrictions
            5. Maintenance responsibility changes
            6. Any other significant differences
            
            Comparison:
            """,
            input_variables=["contract1", "contract2"]
        )
        
        # è·å–ä¸¤ä»½åˆåŒçš„æ‘˜è¦
        summary1 = self.summarize_contract(pdf_path1, "comprehensive")
        summary2 = self.summarize_contract(pdf_path2, "comprehensive")
        
        chain = LLMChain(llm=self.llm, prompt=prompt)
        comparison = chain.run(contract1=summary1, contract2=summary2)
        
        return comparison
    
    def extract_key_information(self) -> Dict:
        """
        æå–åˆåŒå…³é”®ä¿¡æ¯åˆ°ç»“æ„åŒ–æ ¼å¼ï¼ˆä¼˜å…ˆä»æ‘˜è¦æå–ï¼‰
        
        Returns:
            åŒ…å«å…³é”®ä¿¡æ¯çš„å­—å…¸
        """
        # è‹¥æœªåŠ è½½å‘é‡åº“ï¼Œä½†å·²æœ‰æ–‡æ¡£ï¼Œä¹Ÿå¯ç›´æ¥ç”Ÿæˆæ‘˜è¦
        if not self.documents:
            return {"error": "No contract loaded"}

        # å…ˆç”Ÿæˆç»¼åˆæ‘˜è¦
        summary_text = self.summarize_contract(summary_type="comprehensive")

        # åŸºäºæ‘˜è¦çš„ç»“æ„åŒ–æå–ï¼ˆJSONè¾“å‡ºï¼‰
        template = """
        You are extracting key information from a rental contract summary.
        Only use the provided Summary content. Do not assume missing details.
        Use specific numbers and dates from the Summary.
        If a field is not present, return exactly "Not mentioned".

        Summary:
        {summary}

        Extract and return a compact JSON object with these keys:
        - rent_amount: string
        - lease_duration: string
        - security_deposit: string
        - payment_due_date: string
        - late_fee: string
        - pet_policy: string
        - maintenance: string
        - termination: string
        - utilities: string
        - parking: string
        """

        prompt = PromptTemplate(template=template, input_variables=["summary"])
        chain = LLMChain(llm=self.llm, prompt=prompt)
        raw = chain.run(summary=summary_text)

        # å°è¯•è§£æJSONï¼›å¤±è´¥åˆ™å›é€€ä¸ºå…¨éƒ¨å­—æ®µ"Not mentioned"
        import json, re
        try:
            # å¯èƒ½æ¨¡å‹è¿”å›åŒ…å«ä»£ç å—ï¼Œå…ˆæŠ½å–JSONç‰‡æ®µ
            match = re.search(r"\{[\s\S]*\}", raw)
            data = json.loads(match.group(0) if match else raw)
        except Exception:
            data = {}

        # ç»Ÿä¸€å­—æ®µä¸å›é€€å€¼
        keys = [
            "rent_amount","lease_duration","security_deposit","payment_due_date",
            "late_fee","pet_policy","maintenance","termination","utilities","parking"
        ]
        extracted_info = {k: (str(data.get(k, "")).strip() or "Not mentioned") for k in keys}

        # å¯¹ç¼ºå¤±é¡¹è¿›è¡Œæ£€ç´¢å¼å›å¡«ï¼ˆè‹¥å‘é‡åº“å¯ç”¨ï¼‰
        if self.vectorstore:
            fallback_queries = {
                "rent_amount": "What is the monthly rent amount? Use exact amount.",
                "lease_duration": "What is the lease duration? Use exact months/years.",
                "security_deposit": "What is the security deposit amount? Use exact amount.",
                "payment_due_date": "On what date each month is rent due? Use exact day/date.",
                "late_fee": "What is the late payment fee or penalty? Use exact amount/terms.",
                "pet_policy": "What is the pet policy? Are pets allowed? State policy briefly.",
                "maintenance": "What are landlord and tenant maintenance responsibilities? Summarize briefly.",
                "termination": "What are the lease termination or early termination conditions?",
                "utilities": "Who pays utilities (water, electricity, gas, etc.)?",
                "parking": "What parking arrangements or spaces are provided?"
            }

            for k, v in extracted_info.items():
                if v == "Not mentioned":
                    qa = self.ask_question(fallback_queries[k], use_compression=False)
                    ans = qa.get("answer", "").strip()
                    if ans and ans.lower() not in {"not mentioned", "unknown", "not specified"}:
                        extracted_info[k] = self._simplify_answer(ans, k)

        return extracted_info
    

    def extract_key_information_parallel(self) -> Dict:
        """
        åŸºäºæ‘˜è¦çš„å•æ¬¡ç»“æ„åŒ–æå–ï¼ˆæ›´å¿«æ›´ç¨³å®šï¼‰
        
        Returns:
            åŒ…å«å…³é”®ä¿¡æ¯çš„å­—å…¸
        """
        if not self.documents:
            return {"error": "No contract loaded"}

        # å¤ç”¨ç»¼åˆæ‘˜è¦ + JSONæå–ï¼ˆä¸éå¹¶è¡Œç‰ˆæœ¬ä¸€è‡´ï¼‰
        summary_text = self.summarize_contract(summary_type="comprehensive")
        template = """
        From the Summary below, extract key rental contract information.
        Use exact numbers/dates only from the Summary. If missing, return "Not mentioned".

        Summary:
        {summary}

        Return JSON with keys:
        rent_amount, lease_duration, security_deposit, payment_due_date,
        late_fee, pet_policy, maintenance, termination, utilities, parking
        """
        prompt = PromptTemplate(template=template, input_variables=["summary"])
        chain = LLMChain(llm=self.llm, prompt=prompt)
        raw = chain.run(summary=summary_text)

        import json, re
        try:
            match = re.search(r"\{[\s\S]*\}", raw)
            data = json.loads(match.group(0) if match else raw)
        except Exception:
            data = {}

        keys = [
            "rent_amount","lease_duration","security_deposit","payment_due_date",
            "late_fee","pet_policy","maintenance","termination","utilities","parking"
        ]
        info = {k: (str(data.get(k, "")).strip() or "Not mentioned") for k in keys}

        # å¹¶è¡Œç‰ˆæœ¬ä¹Ÿè¿›è¡Œç¼ºå¤±é¡¹å›å¡«ï¼ˆè‹¥å‘é‡åº“å¯ç”¨ï¼‰
        if self.vectorstore:
            fallback_queries = {
                "rent_amount": "What is the monthly rent amount? Use exact amount.",
                "lease_duration": "What is the lease duration? Use exact months/years.",
                "security_deposit": "What is the security deposit amount? Use exact amount.",
                "payment_due_date": "On what date each month is rent due? Use exact day/date.",
                "late_fee": "What is the late payment fee or penalty? Use exact amount/terms.",
                "pet_policy": "What is the pet policy? Are pets allowed? State policy briefly.",
                "maintenance": "What are landlord and tenant maintenance responsibilities? Summarize briefly.",
                "termination": "What are the lease termination or early termination conditions?",
                "utilities": "Who pays utilities (water, electricity, gas, etc.)?",
                "parking": "What parking arrangements or spaces are provided?"
            }

            # é¡ºåºå›å¡«ï¼ˆé¿å…APIè¿‡å¤šå¹¶å‘ï¼‰
            for k, v in info.items():
                if v == "Not mentioned":
                    qa = self.ask_question(fallback_queries[k], use_compression=False)
                    ans = qa.get("answer", "").strip()
                    if ans and ans.lower() not in {"not mentioned", "unknown", "not specified"}:
                        info[k] = self._simplify_answer(ans, k)

        return info
        with ThreadPoolExecutor(max_workers=10) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_key = {
                executor.submit(
                    self.ask_question, 
                    query, 
                    use_compression=False  # å…³é—­å‹ç¼©ï¼Œè¿›ä¸€æ­¥æé€Ÿ
                ): key
                for key, query in extraction_queries.items()
            }
            
            # æ”¶é›†ç»“æœï¼ˆæŒ‰å®Œæˆé¡ºåºï¼‰
            completed = 0
            total = len(extraction_queries)
            
            for future in as_completed(future_to_key):
                key = future_to_key[future]
                try:
                    result = future.result()
                    answer = result["answer"]
                    
                    # åå¤„ç†ï¼šå°†æ¨¡ç³Šæˆ–æœªçŸ¥ç­”æ¡ˆæ›¿æ¢ä¸º "Not mentioned"
                    answer_lower = answer.lower().strip()
                    uncertain_phrases = [
                        "i don't know",
                        "i do not know",
                        "not found",
                        "cannot find",
                        "unable to find",
                        "no information",
                        "not specified",
                        "not mentioned",
                        "not available",
                        "not provided",
                        "unclear",
                        "unknown"
                    ]
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸ç¡®å®šçš„ç­”æ¡ˆ
                    if any(phrase in answer_lower for phrase in uncertain_phrases) or len(answer.strip()) < 3:
                        extracted_info[key] = "Not mentioned"
                    else:
                        # ç®€åŒ–ç­”æ¡ˆï¼Œä½¿å…¶æ›´ç®€æ´
                        simplified_answer = self._simplify_answer(answer, key)
                        extracted_info[key] = simplified_answer
                    
                    completed += 1
                    print(f"âœ… [{completed}/{total}] Extracted: {key}")
                except Exception as e:
                    extracted_info[key] = "Not mentioned"
                    completed += 1
                    print(f"âŒ [{completed}/{total}] Failed: {key} - {e}")
        
        elapsed = time.time() - start_time
        print(f"ğŸ‰ All extractions completed in {elapsed:.2f} seconds")
        
        return extracted_info

    def _simplify_answer(self, answer: str, key: str) -> str:
        """
        ç®€åŒ–ç­”æ¡ˆï¼Œä½¿å…¶æ›´ç®€æ´ï¼Œé¿å…é•¿å¥å­ï¼Œä½†ä¿ç•™å…³é”®ç»†èŠ‚
        
        Args:
            answer: åŸå§‹ç­”æ¡ˆ
            key: å­—æ®µé”®å
            
        Returns:
            ç®€åŒ–çš„ç­”æ¡ˆ
        """
        import re
        
        # å¦‚æœç­”æ¡ˆå·²ç»æ˜¯ç®€çŸ­çš„ï¼Œç›´æ¥è¿”å›
        if len(answer.strip()) <= 60:
            return answer.strip()
        
        # æ ¹æ®ä¸åŒå­—æ®µç±»å‹è¿›è¡Œç®€åŒ–
        if key == "rent_amount":
            # æå–é‡‘é¢
            amounts = re.findall(r'\$[\d,]+(?:\.\d{2})?', answer)
            if amounts:
                return amounts[0]
            # æŸ¥æ‰¾æ•°å­—é‡‘é¢
            numbers = re.findall(r'\b\d{1,3}(?:,\d{3})*(?:\.\d{2})?\b', answer)
            if numbers:
                return f"${numbers[0]}"
                
        elif key == "lease_duration":
            # æå–æ—¶é—´æ®µ
            durations = re.findall(r'\b\d+\s+(?:month|year|week|day)s?\b', answer, re.IGNORECASE)
            if durations:
                return durations[0]
            # æŸ¥æ‰¾æ•°å­—+æ—¶é—´å•ä½
            time_patterns = re.findall(r'\b\d+\s*(?:month|year|week|day|yr|mo|wk|dy)s?\b', answer, re.IGNORECASE)
            if time_patterns:
                return time_patterns[0]
                
        elif key == "security_deposit":
            # æå–æŠ¼é‡‘é‡‘é¢
            amounts = re.findall(r'\$[\d,]+(?:\.\d{2})?', answer)
            if amounts:
                return amounts[0]
            numbers = re.findall(r'\b\d{1,3}(?:,\d{3})*(?:\.\d{2})?\b', answer)
            if numbers:
                return f"${numbers[0]}"
                
        elif key == "payment_due_date":
            # æå–æ—¥æœŸ
            dates = re.findall(r'\b\d{1,2}(?:st|nd|rd|th)?\b', answer)
            if dates:
                return f"{dates[0]}th of each month"
            # æŸ¥æ‰¾"first", "last"ç­‰
            day_words = re.findall(r'\b(?:first|last|1st|15th|30th|31st)\b', answer, re.IGNORECASE)
            if day_words:
                return f"{day_words[0].lower()} of month"
                
        elif key == "late_fee":
            # æå–ç½šæ¬¾é‡‘é¢æˆ–ç™¾åˆ†æ¯”
            amounts = re.findall(r'\$[\d,]+(?:\.\d{2})?|\d+(?:\.\d+)?%', answer)
            if amounts:
                return amounts[0]
            numbers = re.findall(r'\b\d+(?:\.\d+)?%?\b', answer)
            if numbers:
                return numbers[0] + ("%" if "%" in answer else "")
                
        elif key == "pet_policy":
            # ç®€åŒ–å® ç‰©æ”¿ç­–ï¼Œä½†ä¿ç•™å…³é”®ç»†èŠ‚
            if "not allowed" in answer.lower() or "no pets" in answer.lower():
                return "No pets allowed"
            elif "allowed" in answer.lower() or "permitted" in answer.lower():
                # æŸ¥æ‰¾æŠ¼é‡‘ä¿¡æ¯
                deposits = re.findall(r'\$[\d,]+(?:\.\d{2})?\s*(?:deposit|fee)', answer, re.IGNORECASE)
                if deposits:
                    return f"Pets allowed with {deposits[0]} deposit"
                else:
                    return "Pets allowed"
            elif "deposit" in answer.lower():
                deposits = re.findall(r'\$[\d,]+', answer)
                if deposits:
                    return f"Pet deposit: {deposits[0]}"
                    
        elif key == "utilities":
            # ä¿ç•™ utilities çš„å…·ä½“ç»†èŠ‚
            utilities_mentioned = []
            
            # æŸ¥æ‰¾å¸¸è§çš„å…¬ç”¨äº‹ä¸šé¡¹ç›®
            utility_types = ['water', 'electricity', 'gas', 'electric', 'power', 'heating', 'cooling', 'internet', 'cable', 'trash', 'sewage', 'garbage']
            
            for utility in utility_types:
                if utility in answer.lower():
                    utilities_mentioned.append(utility.title())
            
            if utilities_mentioned:
                # ç¡®å®šè°è´Ÿè´£
                if "tenant" in answer.lower() and "landlord" not in answer.lower():
                    return f"Tenant pays: {', '.join(utilities_mentioned)}"
                elif "landlord" in answer.lower() and "tenant" not in answer.lower():
                    return f"Landlord pays: {', '.join(utilities_mentioned)}"
                elif "shared" in answer.lower() or "split" in answer.lower():
                    return f"Shared: {', '.join(utilities_mentioned)}"
                elif "included" in answer.lower():
                    return f"Included in rent: {', '.join(utilities_mentioned)}"
                else:
                    return f"Utilities: {', '.join(utilities_mentioned)}"
            else:
                # å¦‚æœæ²¡æ‰¾åˆ°å…·ä½“é¡¹ç›®ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
                if "tenant" in answer.lower() and "landlord" not in answer.lower():
                    return "Tenant pays utilities"
                elif "landlord" in answer.lower() and "tenant" not in answer.lower():
                    return "Landlord pays utilities"
                elif "shared" in answer.lower() or "split" in answer.lower():
                    return "Utilities shared/split"
                elif "included" in answer.lower():
                    return "Utilities included in rent"
                
        elif key == "parking":
            # ä¿ç•™åœè½¦çš„ç»†èŠ‚
            if "included" in answer.lower():
                return "Parking included"
            elif "available" in answer.lower():
                spaces = re.findall(r'\b\d+\s*(?:space|spot|car)s?\b', answer, re.IGNORECASE)
                if spaces:
                    return f"Parking available: {spaces[0]}"
                else:
                    return "Parking available"
            spaces = re.findall(r'\b\d+\s*(?:space|spot|car)s?\b', answer, re.IGNORECASE)
            if spaces:
                return spaces[0]
                
        elif key == "maintenance":
            # ä¿ç•™ç»´æŠ¤è´£ä»»çš„ç»†èŠ‚
            if "tenant" in answer.lower() and "landlord" not in answer.lower():
                return "Tenant responsible for maintenance"
            elif "landlord" in answer.lower() and "tenant" not in answer.lower():
                return "Landlord responsible for maintenance"
            elif "shared" in answer.lower():
                return "Maintenance responsibilities shared"
            # å°è¯•æå–å…·ä½“çš„ç»´æŠ¤é¡¹ç›®
            maintenance_items = []
            maint_types = ['repairs', 'fixtures', 'appliances', 'plumbing', 'electrical', 'heating', 'cooling', 'painting']
            for item in maint_types:
                if item in answer.lower():
                    maintenance_items.append(item.title())
            if maintenance_items:
                return f"Maintenance: {', '.join(maintenance_items)}"
                
        elif key == "termination":
            # ä¿ç•™ç»ˆæ­¢æ¡ä»¶çš„ç»†èŠ‚
            if "notice" in answer.lower():
                notices = re.findall(r'\b\d+\s*(?:day|week|month)s?\s*notice\b', answer, re.IGNORECASE)
                if notices:
                    return f"{notices[0]} notice required"
            # æŸ¥æ‰¾æå‰ç»ˆæ­¢æ¡æ¬¾
            early_terms = re.findall(r'(?:break|terminate|early).{0,50}(?:fee|penalty|charge)', answer, re.IGNORECASE)
            if early_terms:
                fees = re.findall(r'\$[\d,]+', early_terms[0])
                if fees:
                    return f"Early termination fee: {fees[0]}"
                    
        # å¯¹äºå…¶ä»–é•¿ç­”æ¡ˆï¼Œå°è¯•æ›´å¥½åœ°æ¦‚æ‹¬è€Œä¸æ˜¯ç®€å•æˆªæ–­
        simplified = answer.strip()
        if len(simplified) > 60:
            # æå–å…³é”®ä¿¡æ¯æ¨¡å¼
            # æŸ¥æ‰¾é‡‘é¢
            amounts = re.findall(r'\$[\d,]+(?:\.\d{2})?', simplified)
            # æŸ¥æ‰¾ç™¾åˆ†æ¯”
            percentages = re.findall(r'\d+(?:\.\d+)?%', simplified)
            # æŸ¥æ‰¾æ—¥æœŸ
            dates = re.findall(r'\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b', simplified)
            # æŸ¥æ‰¾æ—¶é—´æ®µ
            periods = re.findall(r'\b\d+\s+(?:month|year|week|day)s?\b', simplified, re.IGNORECASE)
            
            key_info = amounts + percentages + dates + periods
            
            if key_info:
                # å¦‚æœæœ‰å…³é”®ä¿¡æ¯ï¼Œæ„å»ºç®€æ´æ‘˜è¦
                summary_parts = []
                if amounts:
                    summary_parts.append(f"Amount: {', '.join(amounts[:2])}")  # æœ€å¤šæ˜¾ç¤º2ä¸ªé‡‘é¢
                if percentages:
                    summary_parts.append(f"Rate: {', '.join(percentages[:1])}")
                if dates:
                    summary_parts.append(f"Date: {dates[0]}")
                if periods:
                    summary_parts.append(f"Period: {periods[0]}")
                
                return "; ".join(summary_parts)
            else:
                # å¦‚æœæ²¡æœ‰å…³é”®ä¿¡æ¯ï¼Œå°è¯•æå–å‰ä¸¤ä¸ªå¥å­
                sentences = re.split(r'[.!?]+', simplified)
                meaningful_sentences = [s.strip() for s in sentences if len(s.strip()) > 10][:2]
                if meaningful_sentences:
                    return ". ".join(meaningful_sentences) + "."
                else:
                    # æœ€åæ‰‹æ®µï¼šæ™ºèƒ½æˆªæ–­
                    return simplified[:55] + "..."
        
        return simplified

    def clear_memory(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.memory.clear()
        print("ğŸ§¹ Conversation memory cleared")
    
    def save_vectorstore(self, path: str = "vectorstore"):
        """ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜"""
        if self.vectorstore:
            self.vectorstore.save_local(path)
            print(f"ğŸ’¾ Vector store saved to {path}")
    
    
    
    # åœ¨ langchain_rag_system.py ä¸­ä¿®æ”¹ load_vectorstore æ–¹æ³•

    def load_vectorstore(self, path: str = "vectorstore", allow_dangerous_deserialization: bool = False):
        """ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨
        
        Args:
            path: å‘é‡å­˜å‚¨è·¯å¾„
            allow_dangerous_deserialization: æ˜¯å¦å…è®¸åŠ è½½pickleæ–‡ä»¶ï¼ˆä»…åœ¨ç¡®ä¿¡æ–‡ä»¶å®‰å…¨æ—¶ä½¿ç”¨ï¼‰
        """
        if os.path.exists(path):
            # æ–°ç‰ˆæœ¬LangChainéœ€è¦æ˜¾å¼å…è®¸ååºåˆ—åŒ–
            self.vectorstore = FAISS.load_local(
                path, 
                self.embeddings,
                allow_dangerous_deserialization=allow_dangerous_deserialization
            )
            self.retriever = self.vectorstore.as_retriever(
                search_type="mmr",
                search_kwargs={
                    "k": 5,
                    "fetch_k": 10
                }
            )
            print(f"ğŸ“‚ Vector store loaded from {path}")
        else:
            print(f"âš ï¸ Vector store path not found: {path}")

    def get_statistics(self) -> Dict:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        total_chunks = sum(len(docs) for docs in self.documents.values())
        
        return {
            "loaded_contracts": len(self.documents),
            "total_chunks": total_chunks,
            "vector_store_size": self.vectorstore.index.ntotal if self.vectorstore else 0,
            "memory_size": len(self.memory.buffer) if hasattr(self.memory, 'buffer') else 0,
            "contracts": list(self.contract_metadata.values())
        }
    
    # åœ¨ langchain_rag_system.py çš„ AdvancedContractRAG ç±»ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•

    def clear_all_documents(self):
        """æ¸…ç©ºæ‰€æœ‰å·²åŠ è½½çš„æ–‡æ¡£å’Œå‘é‡å­˜å‚¨
        åœ¨åŠ è½½æ–°æ–‡ä»¶å‰è°ƒç”¨ï¼Œç¡®ä¿ä¸ä¼šæ··åˆä¸åŒçš„åˆåŒ
        """
        # æ¸…ç©ºæ–‡æ¡£
        self.documents.clear()
        self.contract_metadata.clear()
        
        # æ¸…ç©ºå‘é‡å­˜å‚¨
        self.vectorstore = None
        self.retriever = None
        
        # æ¸…ç©ºå¯¹è¯è®°å¿†
        if hasattr(self, 'memory') and self.memory:
            self.memory.clear()
        
        print("ğŸ§¹ Cleared all documents and vector stores")

    def get_current_documents_info(self):
        """è·å–å½“å‰åŠ è½½çš„æ–‡æ¡£ä¿¡æ¯"""
        if not self.documents:
            return "No documents loaded"
        
        info = []
        for doc_path, chunks in self.documents.items():
            info.append(f"ğŸ“„ {Path(doc_path).name}: {len(chunks)} chunks")
        
        return "\n".join(info)

    def ensure_single_document(self, file_path: str):
        """ç¡®ä¿åªæœ‰ä¸€ä¸ªæ–‡æ¡£è¢«åŠ è½½
        
        Args:
            file_path: è¦åŠ è½½çš„æ–‡ä»¶è·¯å¾„
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ä¸ªæ–‡ä»¶
        if len(self.documents) == 1 and str(file_path) in self.documents:
            print(f"âœ… Same document already loaded: {Path(file_path).name}")
            return True
        
        # å¦‚æœæ˜¯ä¸åŒæ–‡ä»¶ï¼Œæ¸…ç©ºä¹‹å‰çš„
        if self.documents and str(file_path) not in self.documents:
            print(f"ğŸ”„ Different document detected, clearing previous data...")
            self.clear_all_documents()
        
        return False
        
        


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    #from config import OPENAI_API_KEY, OPENAI_MODEL
    
    api_key =os.getenv("OPENAI_API_KEY")
    model = os.getenv("OPENAI_MODEL", "gpt-4o")

    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag = AdvancedContractRAG(api_key, model)
    

    # åŠ è½½PDF
    result = rag.load_pdf("documents/contract.pdf")
    print(result)
    
    # ç”Ÿæˆæ‘˜è¦
    summary = rag.summarize_contract(summary_type="key points")
    print("\nğŸ“ Contract Summary:")
    print(summary)
    
    # é—®ç­”
    question = "What is the monthly rent and when is it due?"
    answer = rag.ask_question(question)
    print(f"\nâ“ Q: {question}")
    print(f"ğŸ’¡ A: {answer['answer']}")
    
    # æå–å…³é”®ä¿¡æ¯
    key_info = rag.extract_key_information()
    print("\nğŸ“Š Key Information:")
    for key, value in key_info.items():
        print(f"  {key}: {value}")