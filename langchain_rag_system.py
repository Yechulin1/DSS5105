# advanced_rag_system.py
"""
é«˜çº§åˆåŒç®¡ç†ç³»ç»Ÿ - ä½¿ç”¨LangChainå®ç°
æ”¯æŒPDFè§£æã€åˆåŒæ€»ç»“ã€æ™ºèƒ½é—®ç­”ç­‰åŠŸèƒ½
"""

import os
from typing import List, Dict, Optional, Tuple
import hashlib
import pickle
from datetime import datetime

# LangChainæ ¸å¿ƒç»„ä»¶
from langchain.document_loaders import PyMuPDFLoader, PDFPlumberLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import (
    RetrievalQA, 
    ConversationalRetrievalChain,
    LLMChain
)
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.callbacks import get_openai_callback

# å·¥å…·ç±»
import numpy as np
from pathlib import Path
import json
from dotenv import load_dotenv
load_dotenv()

class AdvancedContractRAG:
    """
    é«˜çº§åˆåŒRAGç³»ç»Ÿ
    ç‰¹æ€§ï¼š
    - å¼ºå¤§çš„PDFè§£æï¼ˆæ”¯æŒå¤æ‚æ ¼å¼ï¼‰
    - æ™ºèƒ½æ–‡æ¡£åˆ†å—
    - è¯­ä¹‰å‘é‡æœç´¢
    - åˆåŒè‡ªåŠ¨æ€»ç»“
    - å¯¹è¯å†å²è®°å¿†
    - å¤šè¯­è¨€æ”¯æŒ
    """
    
    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo", language: str = "en"):
        """
        åˆå§‹åŒ–é«˜çº§RAGç³»ç»Ÿ
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹ (gpt-3.5-turbo, gpt-4ç­‰)
            language: è¯­è¨€è®¾ç½® (en, zhç­‰)
        """
        self.api_key = api_key
        self.model = model
        self.language = language
        
        # åˆå§‹åŒ–OpenAIç»„ä»¶
        self.llm = ChatOpenAI(
            temperature=0.01,
            model=model,
            openai_api_key=api_key,
            max_tokens=500
        )
        
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=api_key
        )
        
        # æ–‡æœ¬åˆ†å‰²å™¨ - æ™ºèƒ½åˆ†å—
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,        # å—å¤§å°
            chunk_overlap=200,     # é‡å éƒ¨åˆ†ä¿æŒä¸Šä¸‹æ–‡
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]  # æ”¯æŒä¸­è‹±æ–‡
        )
        
        # å‘é‡å­˜å‚¨
        self.vectorstore = None
        self.retriever = None
        
        # å¯¹è¯è®°å¿†
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            input_key="question",   # âœ… å‘Šè¯‰ memoryï¼šè¾“å…¥å­—æ®µå« question
            output_key="answer",
            return_messages=True,
            k=5  # è®°ä½æœ€è¿‘5è½®å¯¹è¯
        )
        
        # å­˜å‚¨å·²åŠ è½½çš„æ–‡æ¡£
        self.documents = {}
        self.contract_metadata = {}
        
        # ç¼“å­˜ç›®å½•
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        
    def load_pdf(self, pdf_path: str, use_cache: bool = True) -> Dict:
        """
        åŠ è½½å¹¶è§£æPDFæ–‡ä»¶
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            åŒ…å«è§£æç»“æœçš„å­—å…¸
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            return {"success": False, "error": f"File not found: {pdf_path}"}
        
        # ç¡®ä¿åªæœ‰ä¸€ä¸ªæ–‡æ¡£ï¼ˆæ–°å¢ï¼‰
        if self.ensure_single_document(str(pdf_path)):
            # å¦‚æœæ˜¯åŒä¸€ä¸ªæ–‡ä»¶ä¸”å·²åŠ è½½ï¼Œç›´æ¥è¿”å›
            return {
                "success": True, 
                "message": "Document already loaded",
                "stats": self.contract_metadata.get(str(pdf_path), {})
            }
    
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(pdf_path)
        cache_path = self.cache_dir / f"{cache_key}.pkl"
        
        if use_cache and cache_path.exists():
            print(f"ğŸ“‚ Loading from cache: {cache_path}")
            with open(cache_path, 'rb') as f:
                cached_data = pickle.load(f)
                self.documents[str(pdf_path)] = cached_data['documents']
                self._rebuild_vectorstore()
                return {"success": True, "message": "Loaded from cache", "stats": cached_data['stats']}
        
        print(f"ğŸ“„ Loading PDF: {pdf_path}")
        
        # å°è¯•å¤šç§PDFåŠ è½½å™¨
        documents = None
        loader_used = None
        
        # æ–¹æ³•1: PDFPlumber (æœ€å¥½çš„è¡¨æ ¼æ”¯æŒ)
        try:
            loader = PDFPlumberLoader(str(pdf_path))
            documents = loader.load()
            loader_used = "PDFPlumber"
            print(f"âœ… Successfully loaded with PDFPlumber")
        except Exception as e:
            print(f"âš ï¸ PDFPlumber failed: {e}")
        
        # æ–¹æ³•2: PyMuPDF (æœ€å‡†ç¡®çš„æ–‡æœ¬æå–)
        if documents is None or len(documents) == 0:
            try:
                loader = PyMuPDFLoader(str(pdf_path))
                documents = loader.load()
                loader_used = "PyMuPDF"
                print(f"âœ… Successfully loaded with PyMuPDF")
            except Exception as e:
                print(f"âš ï¸ PyMuPDF failed: {e}")
        
        if documents is None or len(documents) == 0:
            return {"success": False, "error": "Failed to extract text from PDF"}
        
        # æå–å…ƒæ•°æ®
        total_pages = len(documents)
        total_text = " ".join([doc.page_content for doc in documents])
        total_chars = len(total_text)
        
        # æ™ºèƒ½æ–‡æ¡£åˆ†å—
        split_documents = self.text_splitter.split_documents(documents)
        
        # ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®
        for i, doc in enumerate(split_documents):
            doc.metadata.update({
                "source": str(pdf_path),
                "chunk_id": i,
                "loader": loader_used,
                "timestamp": datetime.now().isoformat()
            })
        
        # å­˜å‚¨æ–‡æ¡£
        self.documents[str(pdf_path)] = split_documents
        
        # æ›´æ–°å‘é‡å­˜å‚¨
        self._rebuild_vectorstore()
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "file": pdf_path.name,
            "pages": total_pages,
            "characters": total_chars,
            "chunks": len(split_documents),
            "loader": loader_used,
            "avg_chunk_size": total_chars // len(split_documents) if split_documents else 0
        }
        
        # ç¼“å­˜å¤„ç†ç»“æœ
        if use_cache:
            cache_data = {
                "documents": split_documents,
                "stats": stats,
                "timestamp": datetime.now().isoformat()
            }
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            print(f"ğŸ’¾ Cached to: {cache_path}")
        
        # å­˜å‚¨åˆåŒå…ƒæ•°æ®
        self.contract_metadata[str(pdf_path)] = stats
        
        return {"success": True, "message": f"Successfully loaded {pdf_path.name}", "stats": stats}
    
    def _get_cache_key(self, file_path: Path) -> str:
        """ç”Ÿæˆæ–‡ä»¶ç¼“å­˜é”®"""
        stat = file_path.stat()
        unique_str = f"{file_path}_{stat.st_size}_{stat.st_mtime}"
        return hashlib.md5(unique_str.encode()).hexdigest()
    
    def _rebuild_vectorstore(self):
        """é‡å»ºå‘é‡å­˜å‚¨"""
        all_documents = []
        for docs in self.documents.values():
            all_documents.extend(docs)
        
        if all_documents:
            print(f"ğŸ”„ Building vector store with {len(all_documents)} chunks...")
            self.vectorstore = FAISS.from_documents(
                all_documents,
                self.embeddings
            )
            
            # åˆ›å»ºå¢å¼ºæ£€ç´¢å™¨
            self.retriever = self.vectorstore.as_retriever(
                search_type="mmr",  # Maximum Marginal Relevance
                search_kwargs={
                    "k": 5,  # è¿”å›5ä¸ªæœ€ç›¸å…³çš„å—
                    "fetch_k": 10  # å…ˆè·å–10ä¸ªå€™é€‰
                }
            )
            print(f"âœ… Vector store ready")
    
    def summarize_contract(self, pdf_path: Optional[str] = None, 
                          summary_type: str = "comprehensive") -> str:
        """
        ç”ŸæˆåˆåŒæ‘˜è¦
        
        Args:
            pdf_path: æŒ‡å®šPDFè·¯å¾„ï¼ŒNoneåˆ™æ€»ç»“æ‰€æœ‰å·²åŠ è½½æ–‡æ¡£
            summary_type: æ‘˜è¦ç±»å‹
                - "brief": ç®€çŸ­æ‘˜è¦ï¼ˆ1-2æ®µï¼‰
                - "comprehensive": å…¨é¢æ‘˜è¦ï¼ˆåŒ…å«æ‰€æœ‰å…³é”®æ¡æ¬¾ï¼‰
                - "key_points": å…³é”®ç‚¹åˆ—è¡¨
                
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        if not self.documents:
            return "No documents loaded. Please load a contract first."
        
        # è·å–è¦æ€»ç»“çš„æ–‡æ¡£
        """  if pdf_path and pdf_path in self.documents:
            docs_to_summarize = self.documents[pdf_path]
        else:
            docs_to_summarize = []
            for docs in self.documents.values():
                docs_to_summarize.extend(docs)
        """
        if pdf_path and pdf_path in self.documents:
            docs_to_summarize = self.documents[pdf_path]
        else:
        # æœ€è¿‘ä¸€ä»½
            last_key = next(reversed(self.documents.keys()))
            docs_to_summarize = self.documents[last_key]

        # æ ¹æ®ç±»å‹é€‰æ‹©æç¤ºæ¨¡æ¿
        if summary_type == "brief":
            prompt_template = """
            Provide a brief 1-2 paragraph summary of this rental contract.
            Focus on the most important terms: rent amount, duration, and key obligations.
            
            Contract content:
            {text}
            
            Brief Summary:
            """
        elif summary_type == "key_points":
            prompt_template = """
            Extract and list the key points from this rental contract.
            Format as a numbered list covering:
            1. Rental amount and payment terms
            2. Lease duration and dates
            3. Security deposit details
            4. Maintenance responsibilities
            5. Termination conditions
            6. Important restrictions or rules
            7. Any special clauses
            
            Contract content:
            {text}
            
            Key Points:
            """
        else:  # comprehensive
            prompt_template = """
            Provide a comprehensive summary of this rental contract.
            Include all important sections:
            - Parties and Property Details
            - Financial Terms (rent, deposits, fees)
            - Lease Period and Renewal
            - Responsibilities (tenant vs landlord)
            - Rules and Restrictions
            - Termination and Penalties
            - Special Conditions
            
            Contract content:
            {text}
            
            Comprehensive Summary:
            """
        
        # åˆ›å»ºæ€»ç»“é“¾
        prompt = PromptTemplate(template=prompt_template, input_variables=["text"])
        
        with get_openai_callback() as cb:
            if len(docs_to_summarize) > 10:
                # é•¿æ–‡æ¡£ä½¿ç”¨map_reduceç­–ç•¥
                chain = load_summarize_chain(
                    self.llm,
                    chain_type="map_reduce",
                    map_prompt=prompt,
                    combine_prompt=prompt
                )
            else:
                # çŸ­æ–‡æ¡£ä½¿ç”¨stuffç­–ç•¥
                chain = load_summarize_chain(
                    self.llm,
                    chain_type="stuff",
                    prompt=prompt
                )
            
            summary = chain.run(docs_to_summarize)
            
            print(f"ğŸ“Š Summary generated - Tokens used: {cb.total_tokens}, Cost: ${cb.total_cost:.4f}")
        
        return summary
    
    def ask_question(self, question: str, use_compression: bool = True) -> Dict:

        if not self.vectorstore:
            return {
                "answer": "No contract loaded. Please upload a PDF contract first.",
                "sources": []
            }

        # é€‰æ‹©æ£€ç´¢å™¨ï¼ˆæ˜¯å¦å¼€å¯å‹ç¼©ï¼‰
        if use_compression:
            compressor = LLMChainExtractor.from_llm(self.llm)
            retriever = ContextualCompressionRetriever(
                base_compressor=compressor,
                base_retriever=self.retriever
            )
        else:
            retriever = self.retriever

        # â­ ä¸æŠŠ memory äº¤ç»™é“¾ï¼›æ”¹ä¸ºæ‰‹åŠ¨ä¼  chat_history
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            return_source_documents=True,
            verbose=False,
            output_key="answer"  # ä¸»è¾“å‡ºä¸º answer
        )

        # ä»æœ¬åœ° memory å–å†å²ï¼Œä¼ ç»™é“¾ï¼ˆlist[BaseMessage] / list[str] å‡å¯ï¼‰
        try:
            history_vars = self.memory.load_memory_variables({})
            chat_history = history_vars.get("chat_history", [])
        except Exception:
            chat_history = []

        # æ‰§è¡Œ
        with get_openai_callback() as cb:
            result = qa_chain.invoke({
                "question": question,
                "chat_history": chat_history
            })

        # æ‰‹åŠ¨æŠŠæœ¬è½®é—®ç­”å†™å› memoryï¼ˆåªå­˜ question/answerï¼‰
        try:
            self.memory.save_context({"question": question}, {"answer": result.get("answer", "")})
        except Exception:
            pass

        # æ•´ç†æ¥æº
        sources = []
        for doc in result.get("source_documents", []):
            sources.append({
                "content": (doc.page_content[:200] + "...") if doc.page_content else "",
                "source": doc.metadata.get("source", "Unknown"),
                "page": doc.metadata.get("page", "Unknown")
            })

        return {
            "answer": result.get("answer", ""),
            "sources": sources,
            "tokens_used": cb.total_tokens if "cb" in locals() else 0
        }

    
    
    def compare_contracts(self, pdf_path1: str, pdf_path2: str) -> str:
        """
        æ¯”è¾ƒä¸¤ä»½åˆåŒçš„å·®å¼‚
        
        Args:
            pdf_path1: ç¬¬ä¸€ä»½åˆåŒè·¯å¾„
            pdf_path2: ç¬¬äºŒä»½åˆåŒè·¯å¾„
            
        Returns:
            æ¯”è¾ƒç»“æœ
        """
        if pdf_path1 not in self.documents or pdf_path2 not in self.documents:
            return "Both contracts must be loaded first."
        
        prompt = PromptTemplate(
            template="""
            Compare these two rental contracts and highlight the key differences:
            
            Contract 1:
            {contract1}
            
            Contract 2:
            {contract2}
            
            Provide a detailed comparison covering:
            1. Rent amount differences
            2. Lease term differences
            3. Deposit variations
            4. Different rules or restrictions
            5. Maintenance responsibility changes
            6. Any other significant differences
            
            Comparison:
            """,
            input_variables=["contract1", "contract2"]
        )
        
        # è·å–ä¸¤ä»½åˆåŒçš„æ‘˜è¦
        summary1 = self.summarize_contract(pdf_path1, "comprehensive")
        summary2 = self.summarize_contract(pdf_path2, "comprehensive")
        
        chain = LLMChain(llm=self.llm, prompt=prompt)
        comparison = chain.run(contract1=summary1, contract2=summary2)
        
        return comparison
    
    def extract_key_information(self) -> Dict:
        """
        æå–åˆåŒå…³é”®ä¿¡æ¯åˆ°ç»“æ„åŒ–æ ¼å¼
        
        Returns:
            åŒ…å«å…³é”®ä¿¡æ¯çš„å­—å…¸
        """
        if not self.vectorstore:
            return {"error": "No contract loaded"}
        
        # å®šä¹‰è¦æå–çš„å…³é”®ä¿¡æ¯
        extraction_queries = {
            "rent_amount": "What is the monthly rent amount?",
            "lease_duration": "What is the lease duration or term?",
            "security_deposit": "What is the security deposit amount?",
            "payment_due_date": "When is rent due each month?",
            "late_fee": "What is the late payment fee or penalty?",
            "pet_policy": "What is the pet policy?",
            "maintenance": "What are the maintenance responsibilities?",
            "termination": "What are the early termination conditions?",
            "utilities": "Who is responsible for utilities?",
            "parking": "What are the parking arrangements?"
        }
        
        extracted_info = {}
        
        for key, query in extraction_queries.items():
            result = self.ask_question(query, use_compression=True)
            extracted_info[key] = result["answer"]
        
        return extracted_info
    
    def clear_memory(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.memory.clear()
        print("ğŸ§¹ Conversation memory cleared")
    
    def save_vectorstore(self, path: str = "vectorstore"):
        """ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜"""
        if self.vectorstore:
            self.vectorstore.save_local(path)
            print(f"ğŸ’¾ Vector store saved to {path}")
    
    
    
    # åœ¨ langchain_rag_system.py ä¸­ä¿®æ”¹ load_vectorstore æ–¹æ³•

    def load_vectorstore(self, path: str = "vectorstore", allow_dangerous_deserialization: bool = False):
        """ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨
        
        Args:
            path: å‘é‡å­˜å‚¨è·¯å¾„
            allow_dangerous_deserialization: æ˜¯å¦å…è®¸åŠ è½½pickleæ–‡ä»¶ï¼ˆä»…åœ¨ç¡®ä¿¡æ–‡ä»¶å®‰å…¨æ—¶ä½¿ç”¨ï¼‰
        """
        if os.path.exists(path):
            # æ–°ç‰ˆæœ¬LangChainéœ€è¦æ˜¾å¼å…è®¸ååºåˆ—åŒ–
            self.vectorstore = FAISS.load_local(
                path, 
                self.embeddings,
                allow_dangerous_deserialization=allow_dangerous_deserialization
            )
            self.retriever = self.vectorstore.as_retriever(
                search_type="mmr",
                search_kwargs={
                    "k": 5,
                    "fetch_k": 10
                }
            )
            print(f"ğŸ“‚ Vector store loaded from {path}")
        else:
            print(f"âš ï¸ Vector store path not found: {path}")

    def get_statistics(self) -> Dict:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        total_chunks = sum(len(docs) for docs in self.documents.values())
        
        return {
            "loaded_contracts": len(self.documents),
            "total_chunks": total_chunks,
            "vector_store_size": self.vectorstore.index.ntotal if self.vectorstore else 0,
            "memory_size": len(self.memory.buffer) if hasattr(self.memory, 'buffer') else 0,
            "contracts": list(self.contract_metadata.values())
        }
    
    # åœ¨ langchain_rag_system.py çš„ AdvancedContractRAG ç±»ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•

    def clear_all_documents(self):
        """æ¸…ç©ºæ‰€æœ‰å·²åŠ è½½çš„æ–‡æ¡£å’Œå‘é‡å­˜å‚¨
        åœ¨åŠ è½½æ–°æ–‡ä»¶å‰è°ƒç”¨ï¼Œç¡®ä¿ä¸ä¼šæ··åˆä¸åŒçš„åˆåŒ
        """
        # æ¸…ç©ºæ–‡æ¡£
        self.documents.clear()
        self.contract_metadata.clear()
        
        # æ¸…ç©ºå‘é‡å­˜å‚¨
        self.vectorstore = None
        self.retriever = None
        
        # æ¸…ç©ºå¯¹è¯è®°å¿†
        if hasattr(self, 'memory') and self.memory:
            self.memory.clear()
        
        print("ğŸ§¹ Cleared all documents and vector stores")

    def get_current_documents_info(self):
        """è·å–å½“å‰åŠ è½½çš„æ–‡æ¡£ä¿¡æ¯"""
        if not self.documents:
            return "No documents loaded"
        
        info = []
        for doc_path, chunks in self.documents.items():
            info.append(f"ğŸ“„ {Path(doc_path).name}: {len(chunks)} chunks")
        
        return "\n".join(info)

    def ensure_single_document(self, file_path: str):
        """ç¡®ä¿åªæœ‰ä¸€ä¸ªæ–‡æ¡£è¢«åŠ è½½
        
        Args:
            file_path: è¦åŠ è½½çš„æ–‡ä»¶è·¯å¾„
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ä¸ªæ–‡ä»¶
        if len(self.documents) == 1 and str(file_path) in self.documents:
            print(f"âœ… Same document already loaded: {Path(file_path).name}")
            return True
        
        # å¦‚æœæ˜¯ä¸åŒæ–‡ä»¶ï¼Œæ¸…ç©ºä¹‹å‰çš„
        if self.documents and str(file_path) not in self.documents:
            print(f"ğŸ”„ Different document detected, clearing previous data...")
            self.clear_all_documents()
        
        return False
        
        


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    #from config import OPENAI_API_KEY, OPENAI_MODEL
    
    api_key =os.getenv("OPENAI_API_KEY")
    model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")

    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag = AdvancedContractRAG(api_key, model)
    

    # åŠ è½½PDF
    result = rag.load_pdf("documents/contract.pdf")
    print(result)
    
    # ç”Ÿæˆæ‘˜è¦
    summary = rag.summarize_contract(summary_type="key_points")
    print("\nğŸ“ Contract Summary:")
    print(summary)
    
    # é—®ç­”
    question = "What is the monthly rent and when is it due?"
    answer = rag.ask_question(question)
    print(f"\nâ“ Q: {question}")
    print(f"ğŸ’¡ A: {answer['answer']}")
    
    # æå–å…³é”®ä¿¡æ¯
    key_info = rag.extract_key_information()
    print("\nğŸ“Š Key Information:")
    for key, value in key_info.items():
        print(f"  {key}: {value}")


